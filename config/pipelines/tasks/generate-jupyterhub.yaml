apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: generate-jupyterhub
spec:
  params:
    - name: DEFAULT_PYTHON_VERSION
      description: Which Python version to use as a default base image if unable to infer better
      default: "3.8"
    - name: url

  workspaces:
    - name: source

  volumes:
    - name: context
      emptyDir: {}

  steps:
    - name: get-base-image
      image: quay.io/thoth-station/s2i-thoth-dev:latest
      workingDir: $(workspaces.source.path)/repo
      env:
        - name: DEFAULT_VERSION
          value: $(params.DEFAULT_PYTHON_VERSION)
      volumeMounts:
        - mountPath: /mnt
          name: context
      script: |
        #!/opt/app-root/bin/python
        import os
        import json
        import yaml
        import sys

        from pipfile import Pipfile

        # List all supported base images here ordered by priority (most favoured on top)
        IMAGE_LIST = [
            ("3.8-elyra", "quay.io/thoth-station/s2i-elyra-custom-notebook", "latest"),
            ("3.8", "quay.io/thoth-station/s2i-custom-py38-notebook", "latest"),
            ("3.6", "quay.io/thoth-station/s2i-custom-notebook", "latest"),
        ]

        DEFAULT_IMAGE = next(
            iter(f"{i}:{t}" for v, i, t in IMAGE_LIST if v == os.getenv("DEFAULT_VERSION")), ""
        )


        def python_from_pipfile(path):
            if not os.path.isfile(f"{path}/Pipfile"):
                return ""

            parsed = Pipfile.load(filename=f"{path}/Pipfile")
            version = parsed.data["_meta"].get("requires", {}).get("python_version", "")

            if "elyra" in parsed.data["default"]:
                return version + "-elyra"
            else:
                return version


        def analyze_overlay(overlay, root):
            print(f'Analyzed overlay \'{overlay.get("name")}\'')
            version = python_from_pipfile(f'{root}/{overlay.get("name")}')
            base_image = overlay.get("build", {}).get("base-image")

            print(f" - Python version used:   {version}")
            print(f" - Preferred base image:  {base_image}")

            return version, base_image


        def guess_best_match(analyzed_overlays):
            for version, base, tag in IMAGE_LIST:
                match = f"{base}:{tag}"
                if version in [o[0] for o in analyzed_overlays]:
                    return match
                    break
                if base in [o[1].split(":")[0] for o in analyzed_overlays]:
                    return match
                    break

            return DEFAULT_IMAGE


        def write_output(answer):
            print("\nBest matching base image: " + answer)
            with open("/mnt/baseImage", "w+") as output:
                output.write(answer)


        def fallback():
            print("Missing or empty aicoe-ci.yaml, analyzing Pipfile only.")
            version = python_from_pipfile(".")
            answer = guess_best_match([(version, "", "")])
            write_output(answer)
            sys.exit(0)


        if not os.path.isfile(".aicoe-ci.yaml"):
            fallback()

        with open(".aicoe-ci.yaml", "r") as stream:
            conf = yaml.safe_load(stream)

        if not isinstance(conf, dict):
            fallback()
        elif conf.get("overlays"):
            overlays, root = (conf.get("overlays"), conf.get("overlays_dir"))
        elif conf.get("build"):
            overlays, root = (conf, "")
        else:
            fallback()

        accumulator = [analyze_overlay(overlay, root) for overlay in overlays]
        answer = guess_best_match(accumulator)
        write_output(answer)

    - name: generate
      image: quay.io/openshift-pipeline/s2i:nightly
      workingDir: $(workspaces.source.path)
      volumeMounts:
        - mountPath: /mnt
          name: context
      script: |
        REPONAME=$(basename $(params.url) .git)
        BASE_IMAGE=$(cat /mnt/baseImage)

        /usr/local/bin/s2i \
        --loglevel=0 \
        build \
        ./repo \
        "$BASE_IMAGE" \
        --as-dockerfile \
        ./Dockerfile \
        --env=UPGRADE_PIP_TO_LATEST=1 \
        --env=THAMOS_RUNTIME_ENVIRONMENT="" \
        --env=THOTH_ADVISE=0 \
        --env=THOTH_ERROR_FALLBACK=1 \
        --env=THOTH_DRY_RUN=1 \
        --env=THAMOS_DEBUG=0 \
        --env=THAMOS_VERBOSE=1 \
        --env=THOTH_PROVENANCE_CHECK=0 \
        --env=GIT_REPO_URL=$(params.url) \
        --env=GIT_REPO_NAME="$REPONAME"
  #
  #  - name: send-for-image-analysis
  #     image: quay.io/thoth-station/s2i-thoth-dev:latest
  #     env:
  #       - name: MANAGEMENT_SECRET
  #         valueFrom:
  #           secretKeyRef:
  #             name: thoth
  #             key: management-api-token
  #       - name: PROMETHEUS_PUSHGATEWAY_HOST
  #         valueFrom:
  #           configMapKeyRef:
  #             key: pushgateway
  #             name: meteor
  #     script: |
  #       #!/opt/app-root/bin/python
  #       import requests
  #       import os
  #       import time
  #       from prometheus_client import CollectorRegistry, Counter, Gauge, push_to_gateway
  #       image_analysis_url = "https://khemenu.thoth-station.ninja/api/v1/analyze?"
  #       headers = {"Content-type": "application/json"}
  #       prometheus_registry = CollectorRegistry()
  #       _THOTH_METRICS_PUSHGATEWAY_URL = os.getenv("PROMETHEUS_PUSHGATEWAY_HOST")
  #       _METRIC_IMAGES_SUBMITTED = Counter(
  #           "meteor_image_analysis_submission",
  #           "Meteor image submitted for analysis.",
  #           [],
  #           registry=prometheus_registry,
  #       )
  #       _METRIC_IMAGE_PUSHED = Counter(
  #           "meteor_image_pushed",
  #           "Meteor image pushed.",
  #           ["image"],
  #           registry=prometheus_registry,
  #       )
  #       _METRIC_IMAGE_PUSH_TIME = Gauge(
  #           "meteor_image_push_time",
  #           "Meteor image push time.",
  #           ["image"],
  #           registry=prometheus_registry,
  #       )
  #       if any(
  #           img in "$(params.base_image)"
  #           for img in ["s2i-custom", "s2i-minimal", "s2i-generic", "s2i-thoth"]
  #       ):
  #           print("Push Step is completed.")
  #           try:
  #               print("Start pushing image push metrics")
  #               _METRIC_IMAGE_PUSH_TIME.labels("$(params.image)").inc(
  #                   float(open("image_push_exe_time", "r").read().strip())
  #               )
  #               _METRIC_IMAGE_PUSHED.labels("$(params.image)").inc()
  #           except Exception as e:
  #               print(f"An error occurred creating the build metrics: {str(e)}")
  #           image_analysis_params = {
  #               "secret": os.getenv("MANAGEMENT_SECRET"),
  #               "image": "$(params.registry)/$(params.registry_org)/$(params.registry_project):latest",
  #               "environment_type": "runtime",
  #               "verify_tls": True,
  #           }
  #           headers = {"Content-type": "application/json"}
  #           image_analysis_response = requests.post(
  #               image_analysis_url, headers=headers, params=image_analysis_params
  #           )
  #           if image_analysis_response.status_code == 202:
  #               _METRIC_IMAGES_SUBMITTED.inc()
  #               print("Successfully submitted for image analysis.")
  #           else:
  #               print("Submit attempt for image analysis has Failed.")
  #               print("Reason:{}".format(image_analysis_response.text))
  #           if _THOTH_METRICS_PUSHGATEWAY_URL:
  #               try:
  #                   print(
  #                       "Submitting metrics to Prometheus pushgateway %r",
  #                       _THOTH_METRICS_PUSHGATEWAY_URL,
  #                   )
  #                   push_to_gateway(
  #                       _THOTH_METRICS_PUSHGATEWAY_URL,
  #                       job="meteor-pipeline",
  #                       registry=prometheus_registry,
  #                   )
  #               except Exception as e:
  #                   print(f"An error occurred pushing the metrics: {str(e)}")
  #           else:
  #               print("Not pushing metrics as Prometheus pushgateway was not provided")
  #     volumeMounts:
  #       - name: varlibcontainers
  #         mountPath: /var/lib/containers
  #
  # sidecars:
  #   - name: send-for-build-analysis
  #     image: quay.io/thoth-station/s2i-thoth-dev:latest
  #     workingDir: $(workspaces.source.path)
  #     env:
  #       - name: image
  #         value: $(params.image)
  #       - name: base_image
  #         value: $(params.base_image)
  #       - name: registry
  #         value: $(params.registry)
  #       - name: registry_org
  #         value: $(params.registry_org)
  #       - name: registry_project
  #         value: $(params.registry_project)
  #       - name: namespace
  #         value: $(context.taskRun.namespace)
  #       - name: PROMETHEUS_PUSHGATEWAY_HOST
  #         valueFrom:
  #           configMapKeyRef:
  #             key: pushgateway
  #             name: meteor
  #     script: |
  #       #!/opt/app-root/bin/python
  #       import requests
  #       import os
  #       import time
  #       import json
  #       from thoth.common import OpenShift
  #       from prometheus_client import CollectorRegistry, Counter, Gauge, push_to_gateway
  #       PROMETHEUS_REGISTRY = CollectorRegistry()
  #       _THOTH_METRICS_PUSHGATEWAY_URL = os.getenv("PROMETHEUS_PUSHGATEWAY_HOST")
  #       _METRIC_BUILDLOG_SUBMITTED = Counter(
  #           "meteor_image_build_log_submission",
  #           "Meteor image build submitted for log analysis.",
  #           [],
  #           registry=PROMETHEUS_REGISTRY,
  #       )
  #       _METRIC_BUILD_COMPLETED = Counter(
  #           "meteor_image_build_completion",
  #           "Meteor image build completed.",
  #           ["image"],
  #           registry=PROMETHEUS_REGISTRY,
  #       )
  #       _METRIC_BUILD_TIME = Gauge(
  #           "meteor_image_build_time",
  #           "Meteor image build time.",
  #           ["image"],
  #           registry=PROMETHEUS_REGISTRY,
  #       )
  #       openshift = OpenShift()
  #       url_host = openshift.openshift_api_url
  #       url_token = openshift.token
  #       pod_id = os.getenv("HOSTNAME")
  #       namespace = os.getenv("namespace")
  #       build_analysis_url = "https://khemenu.thoth-station.ninja/api/v1/build-analysis"
  #       headers = {"Content-type": "application/json"}
  #       log = ""
  #       build_params = {"container": "step-build"}
  #       required_cont = {"state": "running"}
  #       status_endpoint = "{}/api/v1/namespaces/{}/pods/{}".format(url_host, namespace, pod_id)
  #       while "running" in required_cont["state"]:
  #           print("Waiting for container step-build, It is still running...")
  #           time.sleep(30)
  #           status_response = requests.get(
  #               status_endpoint,
  #               headers={
  #                   "Authorization": "Bearer {}".format(url_token),
  #                   "Content-Type": "application/json",
  #               },
  #               verify=False,
  #               params=build_params,
  #           )
  #           status_response = status_response.json()
  #           if "containerStatuses" in status_response["status"]:
  #               for cont in status_response["status"]["containerStatuses"]:
  #                   if cont["name"] == "step-build":
  #                       required_cont = cont
  #                       break
  #           else:
  #               # logs cant be retrived
  #               required_cont = {"state": "failed"}
  #       if "terminated" in required_cont["state"]:
  #           time.sleep(30)
  #           print("Build Step is completed.")
  #           try:
  #               _METRIC_BUILD_TIME.labels(os.getenv("image")).inc(
  #                   float(open("/gen-source/build_exe_time", "r").read().strip())
  #               )
  #               _METRIC_BUILD_COMPLETED.labels(os.getenv("image")).inc()
  #           except Exception as e:
  #               print(f"An error occurred creating the build metrics: {str(e)}")
  #           log_endpoint = "{}/api/v1/namespaces/{}/pods/{}/log".format(
  #               url_host, namespace, pod_id
  #           )
  #           log_response = requests.get(
  #               log_endpoint,
  #               headers={
  #                   "Authorization": "Bearer {}".format(url_token),
  #                   "Content-Type": "application/json",
  #               },
  #               verify=False,
  #               params=build_params,
  #           )
  #           log = log_response.text
  #           build_analysis = {
  #               "base_image": os.getenv("base_image"),
  #               "build_log": {
  #                   "apiversion": "",
  #                   "kind": '"BuildLog"\n',
  #                   "log": json.dumps(log),
  #                   "metadata": "string",
  #               },
  #               "output_image": "{}/{}/{}:{}".format(
  #                   os.getenv("registry"),
  #                   os.getenv("registry_org"),
  #                   os.getenv("registry_project"),
  #                   "latest",
  #               ),
  #           }
  #           build_analysis_response = requests.post(
  #               build_analysis_url, headers=headers, data=json.dumps(build_analysis)
  #           )
  #           if build_analysis_response.status_code == 202:
  #               print("Successfully submitted for build analysis.")
  #               _METRIC_BUILDLOG_SUBMITTED.inc()
  #           else:
  #               print("Submit attempt for build analysis has Failed.")
  #               print("Reason:{}".format(build_analysis_response.text))
  #       else:
  #           print("Failed to submit for build analysis.")
  #           print("Status Response:", status_response)
  #       if _THOTH_METRICS_PUSHGATEWAY_URL:
  #           try:
  #               print(
  #                   "Submitting metrics to Prometheus pushgateway %r",
  #                   _THOTH_METRICS_PUSHGATEWAY_URL,
  #               )
  #               push_to_gateway(
  #                   _THOTH_METRICS_PUSHGATEWAY_URL,
  #                   job="meteor-pipeline",
  #                   registry=PROMETHEUS_REGISTRY,
  #               )
  #           except Exception as e:
  #               print(f"An error occurred pushing the metrics: {str(e)}")
  #       else:
  #           print("Not pushing metrics as Prometheus pushgateway was not provided")
